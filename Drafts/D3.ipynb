{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "D3",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CCDQS2UCmeY",
        "outputId": "d3a36502-201e-482a-f566-20636ced8ac7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQCoqXlbCoge",
        "outputId": "4117aa74-2863-4056-f19d-074db542a096"
      },
      "source": [
        "with open('/gdrive/My Drive/Projects/Bumblebee/using-pretrained-glove-vectors-example/glove.6B.50d.txt', 'r') as f:\n",
        "  print(f.readline(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the 0.418 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kKOpcZtCpTN"
      },
      "source": [
        "GloVe_path = '/gdrive/My Drive/Projects/Bumblebee/using-pretrained-glove-vectors-example/glove.6B.50d.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrBm4NW9Cp9d"
      },
      "source": [
        "# import numpy as np\n",
        "# # from numpy import arccos\n",
        "# from scipy import spatial\n",
        "# # import matplotlib.pyplot as plt\n",
        "# # from sklearn.manifold import TSNE\n",
        "# # from sklearn.metrics.pairwise import cosine_similarity\n",
        "# # from collections import OrderedDict\n",
        "# import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjqfEL5IzfGD"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from scipy import spatial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNeOyFzjCquE"
      },
      "source": [
        "embeddings_dict = {}\n",
        "with open(GloVe_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        token = values[0]\n",
        "        vector = np.asarray(values[1:], \"float32\")\n",
        "        embeddings_dict[token] = vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pLgLiAZp6Iv"
      },
      "source": [
        "phrases = {\n",
        "          \"hi there\" : \"video1\",  \n",
        "          \"how you doing\" : \"video2\",\n",
        "          \"who are you\" : \"video3\",\n",
        "          \"i am you\" : \"video3\",\n",
        "          \"my name is josh\" : \"video3\",\n",
        "          \"i am kevin\" : \"video3\",\n",
        "          \"i am josh\" : \"vid4\",\n",
        "          \"i josh am\" : \"vid5\",\n",
        "          \"who is kevin\" : \"joe\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kunHlNHLp8ow"
      },
      "source": [
        "Class Vorx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tf01PNQ6p-Cg"
      },
      "source": [
        "class Vorx:\n",
        "\n",
        "  def __init__(self, phrase_pool, phrase, embeddings_dict, size=15):\n",
        "    self.phrase_pool = phrase_pool\n",
        "    self.embeddings_dict = embeddings_dict\n",
        "    self.phrase = phrase\n",
        "    self.size = size\n",
        "    self.embedded_phrase_pool = self.vector_sort(self.embed_phrases()[0])\n",
        "    self.embedded_phrase_mapping = self.embed_phrases()[1]\n",
        "\n",
        "  def __repr__(self):\n",
        "    counter = 0\n",
        "\n",
        "    output = \"\"\n",
        "    output += \"Sorted Embedded Phrase Pool:\"\n",
        "    output += \"\\n\"\n",
        "\n",
        "    for line in self.embedded_phrase_pool:\n",
        "      phrase = self.embedded_phrase_mapping[line]\n",
        "\n",
        "      output += str(counter) + \".\"\n",
        "      output += \"\\t\"\n",
        "      output += phrase\n",
        "      output += \"\\t -> \\t\"\n",
        "      output += str(line)\n",
        "      output += \"\\n\"\n",
        "    \n",
        "      counter += 1\n",
        "    return output\n",
        "\n",
        "  def embed_sentence(self, sentence):\n",
        "    # O(n)\n",
        "    # Embed the sentence by getting the embed for each word\n",
        "    words = sentence.split()\n",
        "    embed = []\n",
        "    for word in words:\n",
        "      embed.append(self.embeddings_dict[word][1])\n",
        "    return embed\n",
        "\n",
        "  def length_normalize(self, words, size=15):\n",
        "    # O(n)\n",
        "    # Make the length of each vector the same to size by adding zeros\n",
        "    length = len(words)\n",
        "    output = words.copy()\n",
        "\n",
        "    if length < self.size:\n",
        "      difference = self.size - length\n",
        "      for i in range(difference):\n",
        "        output.append(0)\n",
        "      return tuple(output)\n",
        "    elif length > self.size:\n",
        "        raise Exception(\"Input length is greater than size. Size is: \" + str(self.size))\n",
        "    else:\n",
        "      return tuple(output)\n",
        "\n",
        "  def embed_phrases(self):\n",
        "    # Embed the phrases in phrase pool, internal\n",
        "    embedded_phrases = []                     # List storing embeds\n",
        "    mapping = {}                              # Dict used to map embed to phrase\n",
        "\n",
        "    for phrase in self.phrase_pool:\n",
        "      embed = self.embed_sentence(phrase)          # Getting embeding of sentence\n",
        "      embed = self.length_normalize(embed, self.size)   # Make embed vector length same\n",
        "    \n",
        "      embedded_phrases.append(embed)\n",
        "      mapping[embed] = phrase\n",
        "\n",
        "    return embedded_phrases, mapping\n",
        "  \n",
        "  def vector_sort(self, vectors):\n",
        "    # Sort the vectors in order from least to greatest\n",
        "    return sorted(vectors, key=lambda x: x[0], reverse=False)\n",
        "  \n",
        "  def cosine_similarity(self, embed1, embed2):\n",
        "    # O(1)\n",
        "    # Gets angle between two vectors -> represents similarity\n",
        "    if len(embed1) != len(embed2):\n",
        "      raise Exception(\"Inputs are not same length. Embed1 is \" + str(len(embed1)) + \" while Embed2 is \" + str(len(embed2)))\n",
        "    return spatial.distance.cosine(embed1, embed2)\n",
        "\n",
        "\n",
        "  def find_phrase(self, phrase):\n",
        "    # Finds most similar phrase from pool to the inputted phrase\n",
        "    # Outputs a tuple (In pool, index)\n",
        "    sentence_in_question = self.embed_sentence(phrase)\n",
        "    sentence_in_question = self.length_normalize(sentence_in_question, self.size)\n",
        "\n",
        "    low = 0\n",
        "    high = len(self.embedded_phrase_pool) - 1\n",
        "\n",
        "    first_time = True\n",
        "\n",
        "    while low <= high:\n",
        "      mid = (high + low) // 2\n",
        "\n",
        "      mid_embedding = self.embedded_phrase_pool[mid]\n",
        "\n",
        "      if mid_embedding == sentence_in_question:\n",
        "        first_time = False\n",
        "        return True, mid\n",
        "      # If x is greater, ignore left half\n",
        "      elif mid_embedding < sentence_in_question:\n",
        "        first_time = False\n",
        "        low = mid + 1\n",
        "\n",
        "      # If x is smaller, ignore right half\n",
        "      else:\n",
        "        first_time = False\n",
        "        high = mid - 1\n",
        "\n",
        "    if first_time:\n",
        "      return False, -1\n",
        "    else:\n",
        "      return False, mid\n",
        "\n",
        "  def ProperNounExtractor(text):\n",
        "      proper_nouns = []\n",
        "      \n",
        "      sentences = nltk.sent_tokenize(text)\n",
        "      for sentence in sentences:\n",
        "          words = nltk.word_tokenize(sentence)\n",
        "          words = [word for word in words if word not in set(stopwords.words('english'))]\n",
        "          tagged = nltk.pos_tag(words)\n",
        "          for (word, tag) in tagged:\n",
        "              if tag == 'NNP': # If the word is a proper noun\n",
        "                  proper_nouns.append(word)\n",
        "      return list(dict.fromkeys(proper_nouns))\n",
        "\n",
        "  def prep_paragraph(self, paragraph):  \n",
        "    proper_nouns = ProperNounExtractor(paragraph)                     # Using NLTK's proper noun tagger\n",
        "    print(\"nouns:\", proper_nouns)\n",
        "\n",
        "    # all_words = re.split(', |\\. |; |: | ', paragraph)               # Split inputted paragraph into stand-alone words\n",
        "    # capitaled_words = [x for x in all_words if x[0].isupper()]      # Find words that start with capital letters\n",
        "\n",
        "    prepared_para = re.split(', |\\. |; |: ', paragraph)             # Split up paragraph by punctuation \n",
        "\n",
        "    for proper_noun in proper_nouns:\n",
        "      for text in prepared_para:\n",
        "        if proper_noun in text:\n",
        "          parts = text.split(\" \" + proper_noun)\n",
        "          for i, t in enumerate(parts):\n",
        "            if t == \"\":\n",
        "              parts[i] = proper_noun\n",
        "          print(parts)\n",
        "    return prepared_para, proper_nouns\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9fB_N634jUY"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrTcSWfl6Tqd"
      },
      "source": [
        "stopwords\n",
        "\n",
        "averaged_perceptron_tagger\n",
        "\n",
        "punktron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vk_BnMAhqCsJ",
        "outputId": "7354b195-bb28-4286-82b5-0c6e193d1f5f"
      },
      "source": [
        "test = Vorx(phrases, \"joe\", embeddings_dict)\n",
        "\n",
        "test.prep_paragraph(\"Hi, my name is Kevin. Kevin is my name\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nouns: ['Hi', 'Kevin', 'Kevin']\n",
            "['Hi']\n",
            "['my name is', 'Kevin']\n",
            "['Kevin is my name']\n",
            "['my name is', 'Kevin']\n",
            "['Kevin is my name']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Hi', 'my name is Kevin', 'Kevin is my name'], ['Hi', 'Kevin', 'Kevin'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 254
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t-PVQdAsj1m"
      },
      "source": [
        "Break apart commas\n",
        "\n",
        "Break apart proper nouns (captial letter)\n",
        "\n",
        "Break apart periods (sentences)\n",
        "\n",
        "Do-later:\n",
        "semicolons\n",
        "colons\n"
      ]
    }
  ]
}