{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "D2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CCDQS2UCmeY",
        "outputId": "4253a8f0-4e00-4231-a24e-331bdce893b1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQCoqXlbCoge",
        "outputId": "c8cd63dc-1ae3-4244-d492-8b830dbd347a"
      },
      "source": [
        "with open('/gdrive/My Drive/Projects/Bumblebee/using-pretrained-glove-vectors-example/glove.6B.50d.txt', 'r') as f:\n",
        "  print(f.readline(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the 0.418 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kKOpcZtCpTN"
      },
      "source": [
        "GloVe_path = '/gdrive/My Drive/Projects/Bumblebee/using-pretrained-glove-vectors-example/glove.6B.50d.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrBm4NW9Cp9d"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import arccos\n",
        "from scipy import spatial\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import OrderedDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNeOyFzjCquE"
      },
      "source": [
        "embeddings_dict = {}\n",
        "with open(GloVe_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        token = values[0]\n",
        "        vector = np.asarray(values[1:], \"float32\")\n",
        "        embeddings_dict[token] = vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB50ArAfPYmM"
      },
      "source": [
        "class Embed:\n",
        "\n",
        "  def __init__(self, phrase_pool, phrase, embeddings_dict, size=15):\n",
        "    self.phrase_pool = phrase_pool\n",
        "    self.embeddings_dict = embeddings_dict\n",
        "    self.phrase = phrase\n",
        "    self.size = size\n",
        "    self.embedded_phrase_pool = self.vector_sort(self.embed_phrases()[0])\n",
        "    self.embedded_phrase_mapping = self.embed_phrases()[1]\n",
        "\n",
        "  def __repr__(self):\n",
        "    counter = 0\n",
        "\n",
        "    output = \"\"\n",
        "    output += \"Sorted Embedded Phrase Pool:\"\n",
        "    output += \"\\n\"\n",
        "\n",
        "    for line in self.embedded_phrase_pool:\n",
        "      phrase = self.embedded_phrase_mapping[line]\n",
        "\n",
        "      output += str(counter) + \".\"\n",
        "      output += \"\\t\"\n",
        "      output += phrase\n",
        "      output += \"\\t -> \\t\"\n",
        "      output += str(line)\n",
        "      output += \"\\n\"\n",
        "    \n",
        "      counter += 1\n",
        "    return output\n",
        "\n",
        "  # O(n)\n",
        "  def embed_sentence(self, sentence):\n",
        "    words = sentence.split()\n",
        "    embed = []\n",
        "    for word in words:\n",
        "      embed.append(self.embeddings_dict[word][1])\n",
        "    return embed\n",
        "\n",
        "  # O(n)\n",
        "  def length_normalize(self, words, size=15):\n",
        "    length = len(words)\n",
        "    output = words.copy()\n",
        "\n",
        "    if length < self.size:\n",
        "      difference = self.size - length\n",
        "      for i in range(difference):\n",
        "        output.append(0)\n",
        "      return tuple(output)\n",
        "    elif length > self.size:\n",
        "        raise Exception(\"Input length is greater than size. Size is: \" + str(self.size))\n",
        "    else:\n",
        "      return tuple(output)\n",
        "\n",
        "  def embed_phrases(self):\n",
        "    embedded_phrases = []                     # List storing embeds\n",
        "    mapping = {}                              # Dict used to map embed to phrase\n",
        "\n",
        "    for phrase in self.phrase_pool:\n",
        "      embed = self.embed_sentence(phrase)          # Getting embeding of sentence\n",
        "      embed = self.length_normalize(embed, self.size)   # Make embed vector length same\n",
        "    \n",
        "      embedded_phrases.append(embed)\n",
        "      mapping[embed] = phrase\n",
        "\n",
        "    return embedded_phrases, mapping\n",
        "  \n",
        "  def vector_sort(self, vector):\n",
        "    return sorted(vector, key=lambda x: x[0], reverse=False)\n",
        "  \n",
        "  # O(1)\n",
        "  def cosine_similarity(self, embed1, embed2):\n",
        "    if len(embed1) != len(embed2):\n",
        "      raise Exception(\"Inputs are not same length. Embed1 is \" + str(len(embed1)) + \" while Embed2 is \" + str(len(embed2)))\n",
        "    return spatial.distance.cosine(embed1, embed2)\n",
        "  \n",
        "  def test(self, test_sentence):\n",
        "    embed = self.embed_sentence(test_sentence)\n",
        "    embed = self.length_normalize(embed)\n",
        "    print(\"Sim to leftest one:\" + str(self.cosine_similarity(embed, self.embedded_phrase_pool[0])))\n",
        "    print(\"Sim to the righest one:\" + str(self.cosine_similarity(embed, self.embedded_phrase_pool[-1])))\n",
        "    print(self.embedded_phrase_pool[-1])\n",
        "\n",
        "  def binary_search(self, phrase):\n",
        "    embed = self.embed_sentence(phrase)\n",
        "    embed = self.length_normalize(embed, self.size)\n",
        "\n",
        "    low = 0\n",
        "    high = len(self.embedded_phrase_pool) - 1\n",
        "\n",
        "    # problem: setting last_sim to 0 makes it always go to the right, thus removing any chance of it looking left\n",
        "    # solution: \n",
        "    # mid = (high + low) // 2\n",
        "    mid = (high + low) // 2\n",
        "    mid_embed = self.embedded_phrase_pool[mid]\n",
        "\n",
        "    right_embed = self.embedded_phrase_pool[mid+1]\n",
        "    left_embed = self.embedded_phrase_pool[mid-1]\n",
        "\n",
        "    right_sim = self.cosine_similarity(right_embed, embed)\n",
        "    left_sim = self.cosine_similarity(left_embed, embed)\n",
        "\n",
        "    similarity = self.cosine_similarity(mid_embed, embed)\n",
        "\n",
        "    if similarity < left_sim:\n",
        "      low = mid + 1\n",
        "      last_similarity = left_sim\n",
        "    elif similarity > right_sim:\n",
        "      high = mid - 1\n",
        "      last_similarity = right_sim\n",
        "    elif similarity == left_sim:\n",
        "      return mid-1\n",
        "    elif similarity == right_sim:\n",
        "      return mid+1\n",
        "\n",
        "    while low <= high:\n",
        "      mid = (high + low) // 2\n",
        "      mid_embed = self.embedded_phrase_pool[mid]\n",
        "\n",
        "      similarity = self.cosine_similarity(mid_embed, embed)\n",
        "      \n",
        "      print((\"current sim is \") + str(similarity))\n",
        "      print(\"last sim is \" + str(last_similarity))\n",
        "      print(\"phrase is: \" + str(self.embedded_phrase_mapping[self.embedded_phrase_pool[mid]]))\n",
        "\n",
        "      if similarity == 0:\n",
        "        return mid\n",
        "      # If x is greater, ignore left half\n",
        "      elif last_similarity < similarity:\n",
        "        low = mid + 1\n",
        "\n",
        "      # If x is smaller, ignore right half\n",
        "      elif last_similarity > similarity:\n",
        "        high = mid - 1\n",
        "\n",
        "      # means x is present at mid\n",
        "      else:\n",
        "          return mid\n",
        "\n",
        "      last_similarity = similarity\n",
        "\n",
        "    if similarity == 0:\n",
        "      return mid\n",
        "    # If we reach here, then the element was not present\n",
        "    return -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3d4AmWnhmM2",
        "outputId": "05a17dc4-3ad2-4ff3-ab14-ea8396fd0ddc"
      },
      "source": [
        "test = Embed(phrases, \"joe\", embeddings_dict)\n",
        "print(test)\n",
        "test.test(\"hi there\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sorted Embedded Phrase Pool:\n",
            "0.\thow you doing\t -> \t(-0.10644, 0.33324, -0.37986, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "1.\twho are you\t -> \t(-0.051277, 0.012516, 0.33324, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "2.\ti am you\t -> \t(0.15255, 0.39805, 0.33324, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "3.\ti am kevin\t -> \t(0.15255, 0.39805, 0.33927, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "4.\ti am josh\t -> \t(0.15255, 0.39805, 0.85859, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "5.\thi there\t -> \t(0.34427, 0.32385, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "6.\tmy name is josh\t -> \t(0.77515, 0.75197, 0.64254, 0.85859, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "\n",
            "Sim to leftest one:0.7079814556986735\n",
            "Sim to the righest one:0.29049627206051676\n",
            "(0.77515, 0.75197, 0.64254, 0.85859, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5dSj-9cQFxE",
        "outputId": "8266ae9a-47b0-48d4-f440-9e526bd2cbaa"
      },
      "source": [
        "test = Embed(phrases, \"joe\", embeddings_dict)\n",
        "print(test)\n",
        "test.cosine_similarity((1,1), (1,1))\n",
        "\n",
        "test.binary_search(\"who are you\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sorted Embedded Phrase Pool:\n",
            "0.\thow you doing\t -> \t(-0.10644, 0.33324, -0.37986, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "1.\twho are you\t -> \t(-0.051277, 0.012516, 0.33324, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "2.\twho is kevin\t -> \t(-0.051277, 0.64254, 0.33927, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "3.\ti am you\t -> \t(0.15255, 0.39805, 0.33324, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "4.\ti am kevin\t -> \t(0.15255, 0.39805, 0.33927, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "5.\ti am josh\t -> \t(0.15255, 0.39805, 0.85859, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "6.\ti josh am\t -> \t(0.15255, 0.85859, 0.39805, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "7.\thi there\t -> \t(0.34427, 0.32385, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "8.\tmy name is josh\t -> \t(0.77515, 0.75197, 0.64254, 0.85859, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "\n",
            "current sim is 0.5808266784376488\n",
            "last sim is 0.4072588812156077\n",
            "phrase is: i josh am\n",
            "current sim is 1.0852812281486992\n",
            "last sim is 0.5808266784376488\n",
            "phrase is: hi there\n",
            "current sim is 0.6420943161712567\n",
            "last sim is 1.0852812281486992\n",
            "phrase is: my name is josh\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yt65TPuBc9MK",
        "outputId": "1f5c29f8-f8d3-4ace-ec42-55d3c2f00bd0"
      },
      "source": [
        "test.binary_search(\"how you doing\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current sim is 0.7079814556986735\n",
            "last sim is 1.0443102401635602\n",
            "phrase is: hi there\n",
            "current sim is 1.0443102401635602\n",
            "last sim is 0.7079814556986735\n",
            "phrase is: i am kevin\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvqfdkbgH4qD"
      },
      "source": [
        "# Phrases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vAIsd7PH6Oq"
      },
      "source": [
        "phrases = {\n",
        "          \"hi there\" : \"video1\",\n",
        "          \"how you doing\" : \"video2\",\n",
        "          \"who are you\" : \"video3\",\n",
        "          \"i am you\" : \"video3\",\n",
        "          \"my name is josh\" : \"video3\",\n",
        "          \"i am kevin\" : \"video3\",\n",
        "          \"i am josh\" : \"vid4\",\n",
        "          \"i josh am\" : \"vid5\",\n",
        "          \"who is kevin\" : \"joe\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IomCmSr7IKej"
      },
      "source": [
        "## Embedding the phrases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfyQT3oE24YX"
      },
      "source": [
        "In this system, the embed lengtyh must be computed everytime, but the angle between the unit vector and the current embed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR-Rx1OiIHVb"
      },
      "source": [
        "def embed_phrases(phrases, size=15):\n",
        "  embedded_phrases = []                     # List storing embeds\n",
        "  mapping = {}                              # Dict used to map embed to phrase\n",
        "\n",
        "  for phrase in phrases:\n",
        "    embed = embed_sentence(phrase)          # Getting embeding of sentence\n",
        "    embed = length_normalize(embed, size)   # Make embed vector length same\n",
        "  \n",
        "    embedded_phrases.append(embed)\n",
        "    mapping[embed] = phrase\n",
        "\n",
        "  return embedded_phrases, mapping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmeAm3J5CHKo"
      },
      "source": [
        "def vector_sort(vector):\n",
        "  return sorted(vector, key=lambda x: x[0], reverse=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THeogx7TcTL8"
      },
      "source": [
        "Sort by storing the values as ints, tehn store those ints as strings ina  dict which point to the phrase which then points to the adress\n",
        "\n",
        "get the angle of the vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLqoonxqD2AX"
      },
      "source": [
        "for line in vector_sort(joe[0]):\n",
        "  print(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0RqliUGEE6W"
      },
      "source": [
        "class Embed:\n",
        "\n",
        "  def __init__(self, phrase_pool, phrase, size=15):\n",
        "    self.phrase_pool = phrase_pool\n",
        "    self.phrase = phrase\n",
        "    self.size = size\n",
        "    self.embedded_phrase_pool = vector_sort(self.embed_phrases()[0])\n",
        "    self.embedded_phrase_mapping = self.embed_phrases()[1]\n",
        "\n",
        "  \n",
        "  def __repr__(self):\n",
        "    counter = 0\n",
        "\n",
        "    output = \"\"\n",
        "    output += \"Sorted Embedded Phrase Pool:\"\n",
        "    output += \"\\n\"\n",
        "\n",
        "    for line in self.embedded_phrase_pool:\n",
        "      phrase = self.embedded_phrase_mapping[line]\n",
        "\n",
        "      output += str(counter) + \".\"\n",
        "      output += \"\\t\"\n",
        "      output += phrase\n",
        "      output += \"\\t -> \\t\"\n",
        "      output += str(line)\n",
        "      output += \"\\n\"\n",
        "    \n",
        "      counter += 1\n",
        "    return output\n",
        "\n",
        "  # O(n)\n",
        "  def embed_sentence(sentence):\n",
        "    words = sentence.split()\n",
        "    embed = []\n",
        "    for word in words:\n",
        "      embed.append(embeddings_dict[word][1])\n",
        "    return embed\n",
        "\n",
        "  # O(n)\n",
        "  def length_normalize(words, size=15):\n",
        "    length = len(words)\n",
        "    output = words.copy()\n",
        "\n",
        "    if length < self.size:\n",
        "      difference = self.size - length\n",
        "      for i in range(difference):\n",
        "        output.append(0)\n",
        "      return tuple(output)\n",
        "    elif length > self.size:\n",
        "        raise Exception(\"Input length is greater than size. Size is: \" + str(self.size))\n",
        "    else:\n",
        "      return tuple(output)\n",
        "\n",
        "  def embed_phrases(self):\n",
        "    embedded_phrases = []                     # List storing embeds\n",
        "    mapping = {}                              # Dict used to map embed to phrase\n",
        "\n",
        "    for phrase in self.phrase_pool:\n",
        "      embed = embed_sentence(phrase)          # Getting embeding of sentence\n",
        "      embed = length_normalize(embed, self.size)   # Make embed vector length same\n",
        "    \n",
        "      embedded_phrases.append(embed)\n",
        "      mapping[embed] = phrase\n",
        "\n",
        "    return embedded_phrases, mapping\n",
        "  \n",
        "  def vector_sort(vector):\n",
        "    return sorted(vector, key=lambda x: x[0], reverse=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0XtOfN9Ffa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "outputId": "5eea9073-84ea-4ab9-d7ec-b709fb9b19c6"
      },
      "source": [
        "test = Embed(phrases, \"joe\", embeddings_dict = e)\n",
        "print(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e320e9b8fc40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"joe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'embeddings_dict' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKZAg8MxmhO-"
      },
      "source": [
        "Latest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBXJbUZsmhDI"
      },
      "source": [
        "class Embed:\n",
        "\n",
        "  def __init__(self, phrase_pool, phrase, embeddings_dict, size=15):\n",
        "    self.phrase_pool = phrase_pool\n",
        "    self.embeddings_dict = embeddings_dict\n",
        "    self.phrase = phrase\n",
        "    self.size = size\n",
        "    self.embedded_phrase_pool = self.vector_sort(self.embed_phrases()[0])\n",
        "    self.embedded_phrase_mapping = self.embed_phrases()[1]\n",
        "\n",
        "  def __repr__(self):\n",
        "    counter = 0\n",
        "\n",
        "    output = \"\"\n",
        "    output += \"Sorted Embedded Phrase Pool:\"\n",
        "    output += \"\\n\"\n",
        "\n",
        "    for line in self.embedded_phrase_pool:\n",
        "      phrase = self.embedded_phrase_mapping[line]\n",
        "\n",
        "      output += str(counter) + \".\"\n",
        "      output += \"\\t\"\n",
        "      output += phrase\n",
        "      output += \"\\t -> \\t\"\n",
        "      output += str(line)\n",
        "      output += \"\\n\"\n",
        "    \n",
        "      counter += 1\n",
        "    return output\n",
        "\n",
        "  # O(n)\n",
        "  def embed_sentence(self, sentence):\n",
        "    words = sentence.split()\n",
        "    embed = []\n",
        "    for word in words:\n",
        "      embed.append(self.embeddings_dict[word][1])\n",
        "    return embed\n",
        "\n",
        "  # O(n)\n",
        "  def length_normalize(self, words, size=15):\n",
        "    length = len(words)\n",
        "    output = words.copy()\n",
        "\n",
        "    if length < self.size:\n",
        "      difference = self.size - length\n",
        "      for i in range(difference):\n",
        "        output.append(0)\n",
        "      return tuple(output)\n",
        "    elif length > self.size:\n",
        "        raise Exception(\"Input length is greater than size. Size is: \" + str(self.size))\n",
        "    else:\n",
        "      return tuple(output)\n",
        "\n",
        "  def embed_phrases(self):\n",
        "    embedded_phrases = []                     # List storing embeds\n",
        "    mapping = {}                              # Dict used to map embed to phrase\n",
        "\n",
        "    for phrase in self.phrase_pool:\n",
        "      embed = self.embed_sentence(phrase)          # Getting embeding of sentence\n",
        "      embed = self.length_normalize(embed, self.size)   # Make embed vector length same\n",
        "    \n",
        "      embedded_phrases.append(embed)\n",
        "      mapping[embed] = phrase\n",
        "\n",
        "    return embedded_phrases, mapping\n",
        "  \n",
        "  def vector_sort(self, vector):\n",
        "    return sorted(vector, key=lambda x: x[0], reverse=False)\n",
        "  \n",
        "  # O(1)\n",
        "  def cosine_similarity(self, embed1, embed2):\n",
        "    if len(embed1) != len(embed2):\n",
        "      raise Exception(\"Inputs are not same length. Embed1 is \" + str(len(embed1)) + \" while Embed2 is \" + str(len(embed2)))\n",
        "    return spatial.distance.cosine(embed1, embed2)\n",
        "  \n",
        "  def test(self, test_sentence):\n",
        "    embed = self.embed_sentence(test_sentence)\n",
        "    embed = self.length_normalize(embed)\n",
        "    print(\"Sim to leftest one:\" + str(self.cosine_similarity(embed, self.embedded_phrase_pool[0])))\n",
        "    print(\"Sim to the righest one:\" + str(self.cosine_similarity(embed, self.embedded_phrase_pool[-1])))\n",
        "    print(self.embedded_phrase_pool[-1])\n",
        "\n",
        "  def binary_search(self, phrase):\n",
        "    sentence_in_question = self.embed_sentence(phrase)\n",
        "    sentence_in_question = self.length_normalize(sentence_in_question, self.size)\n",
        "\n",
        "    low = 0\n",
        "    high = len(self.embedded_phrase_pool) - 1\n",
        "\n",
        "    first_time = True\n",
        "\n",
        "    while low <= high:\n",
        "      mid = (high + low) // 2\n",
        "\n",
        "      mid_embedding = self.embedded_phrase_pool[mid]\n",
        "      # mid_sentence = self.embedded_phrase_mapping[self.embedded_phrase_pool[mid]]\n",
        "      # high_sentence = self.embedded_phrase_mapping[self.embedded_phrase_pool[high]]\n",
        "      # low_sentence = self.embedded_phrase_mapping[self.embedded_phrase_pool[low]]\n",
        "\n",
        "      if mid_embedding == sentence_in_question:\n",
        "        first_time = False\n",
        "        return mid\n",
        "      # If x is greater, ignore left half\n",
        "      elif mid_embedding < sentence_in_question:\n",
        "        first_time = False\n",
        "        low = mid + 1\n",
        "\n",
        "      # If x is smaller, ignore right half\n",
        "      else:\n",
        "        first_time = False\n",
        "        high = mid - 1\n",
        "\n",
        "    if first_time:\n",
        "      return -1\n",
        "    else:\n",
        "      current_position_embed = self.embedded_phrase_pool[mid]\n",
        "      next_position_embed = self.embedded_phrase_pool[mid+1]\n",
        "\n",
        "      sim_current = self.cosine_similarity(sentence_in_question, current_position_embed)\n",
        "      sim_next = self.cosine_similarity(sentence_in_question, next_position_embed)\n",
        "\n",
        "      print(\"sim next:\", str(sim_next), str(next_position_embed))\n",
        "      print(\"sim current:\", str(sim_current), str(current_position_embed))\n",
        "\n",
        "      print(\"word in question:\", str(sentence_in_question))\n",
        "      if sim_current < sim_next:\n",
        "        return mid\n",
        "      else:\n",
        "        return mid+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7h0LVjroKac",
        "outputId": "4f185722-1399-4985-8f2b-341366fbf52f"
      },
      "source": [
        "test = Embed(phrases, \"joe\", embeddings_dict)\n",
        "print(test)\n",
        "\n",
        "test.binary_search(\"who is you\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sorted Embedded Phrase Pool:\n",
            "0.\thow you doing\t -> \t(-0.10644, 0.33324, -0.37986, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "1.\twho are you\t -> \t(-0.051277, 0.012516, 0.33324, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "2.\twho is kevin\t -> \t(-0.051277, 0.64254, 0.33927, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "3.\ti am you\t -> \t(0.15255, 0.39805, 0.33324, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "4.\ti am kevin\t -> \t(0.15255, 0.39805, 0.33927, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "5.\ti am josh\t -> \t(0.15255, 0.39805, 0.85859, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "6.\ti josh am\t -> \t(0.15255, 0.85859, 0.39805, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "7.\thi there\t -> \t(0.34427, 0.32385, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "8.\tmy name is josh\t -> \t(0.77515, 0.75197, 0.64254, 0.85859, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "\n",
            "sim next: 0.08565741878918387 (0.15255, 0.39805, 0.33324, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "sim current: 2.7038278017865736e-05 (-0.051277, 0.64254, 0.33927, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "word in question: (-0.051277, 0.64254, 0.33324, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-kUid7RohL6"
      },
      "source": [
        "assumption: That the sorted phrase pool puts closest sentences together\n",
        "is this correct? can this be assumed?"
      ]
    }
  ]
}